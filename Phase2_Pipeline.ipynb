{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d183b94",
   "metadata": {},
   "source": [
    "# Phase 2: Story2Audio Pipeline\n",
    "\n",
    "This notebook implements the Story2Audio pipeline for Phase 2 of the NLP project. It:\n",
    "- Preprocesses a story into chunks.\n",
    "- Enhances chunks using Phi-2 for storytelling tone.\n",
    "- Generates audio using Bark TTS.\n",
    "- Stitches audio into a final .mp3 file.\n",
    "\n",
    "**Requirements**:\n",
    "- Python 3.8+\n",
    "- FFmpeg installed and added to PATH\n",
    "- Dependencies: transformers, torch, bark, pydub, scipy\n",
    "- Hardware: CPU (GPU recommended for faster inference)\n",
    "\n",
    "**Output**: outputs/final_story.mp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177462b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "print(np.array([1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0614d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from src.preprocess import chunk_story\n",
    "from src.enhancer_local import StoryEnhancer\n",
    "from src.kokoro_tts import text_to_coqui_audio\n",
    "from src.utils import combine_audio\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911c434",
   "metadata": {},
   "source": [
    "# # Step 1: Load and Chunk Story\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e85320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅Story loaded successfully\n",
      "INFO:__main__:✅Story split into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load sample story\n",
    "    with open('sample_story.txt', 'r', encoding='utf-8') as f:\n",
    "        story_text = f.read()\n",
    "    logger.info('✅Story loaded successfully')\n",
    "\n",
    "    # Chunk story (~150 words per chunk)\n",
    "    chunks = chunk_story(story_text, chunk_size=150)\n",
    "    logger.info(f'✅Story split into {len(chunks)} chunks')\n",
    "except Exception as e:\n",
    "    logger.error(f'Error in preprocessing: {e}')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492bc0f",
   "metadata": {},
   "source": [
    "# # Step 2: Enhance Chunks with tiiuae/falcon-rw-1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0f8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Semester 8\\NLP\\Project\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a forest full of mysteries, there was a story of a boy and his cat. At first, the boy was afraid and he thought it would be a disaster to go out into the forest. When finally he was on the place he decided to test a magical cat, he caught her by her tail and the cat was very scared. She jumped away and disappeared immediately. Then the boy came closer and caught her by her neck. He looked into the eyes of the cat in silence and that was the beginning of a great friendship between them. The boy kept doing experiments in the forest and his friends were watching very patiently. Then a magic book fell into the forest accidentally and the cat went to get it. The boys started to\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"tiiuae/falcon-rw-1b\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",              # Let accelerate handle device assignment\n",
    "    offload_folder=\"offload\"        # Needed if model is too large for GPU\n",
    ")\n",
    "\n",
    "# Correct: no `device` argument\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt = \"Once upon a time in a forest full of mysteries,\"\n",
    "results = text_generator(prompt, max_length=150, do_sample=True)\n",
    "print(results[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf4e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n",
      "INFO:src.enhancer_local:Initialized StoryEnhancer locally with model: tiiuae/falcon-rw-1b\n",
      "INFO:__main__:StoryEnhancer initialized with Hugging Face API\n",
      "INFO:src.enhancer_local:Tokenized input length: 164 tokens\n",
      "INFO:__main__:Enhanced chunk 1/1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize enhancer\n",
    "    enhancer = StoryEnhancer()\n",
    "    logger.info('✅StoryEnhancer initialized Locally')\n",
    "\n",
    "    # Enhance each chunk\n",
    "    enhanced_chunks = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        enhanced = enhancer.enhance_chunk(chunk)\n",
    "        enhanced_chunks.append(enhanced)\n",
    "        logger.info(f'✅Enhanced chunk {idx + 1}/{len(chunks)}')\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f'Error in enhancement: {e}')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971a494",
   "metadata": {},
   "source": [
    "# # Step 3: Generate Audio with Kokoro_tts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66a4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in audio generation: name 'enhanced_chunks' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'enhanced_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/temp\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m     audio_files \u001b[38;5;241m=\u001b[39m text_to_coqui_audio(\u001b[43menhanced_chunks\u001b[49m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs/temp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated audio files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enhanced_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.makedirs('outputs/temp', exist_ok=True)\n",
    "    audio_files = text_to_coqui_audio(enhanced_chunks, output_dir='outputs/temp')\n",
    "    logger.info(f'Generated audio files: {audio_files}')\n",
    "except Exception as e:\n",
    "    logger.error(f'Error in audio generation: {e}')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfec94",
   "metadata": {},
   "source": [
    "# # Step 4: Stitch Audio into Final MP3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50505576",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Combine audio files\n",
    "    output_path = 'outputs/final_story.mp3'\n",
    "    combine_audio(audio_files, output_path)\n",
    "    logger.info(f'Audio generated: {output_path}')\n",
    "except Exception as e:\n",
    "    logger.error(f'Error in audio stitching: {e}')\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c8720d",
   "metadata": {},
   "source": [
    "# # Step 5: Verify Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf496479",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_path):\n",
    "    logger.info('✅ Verification: Final audio file exists and is playable')\n",
    "else:\n",
    "    logger.error('❌ Verification: Final audio file not found')\n",
    "    raise FileNotFoundError(f'Output file {output_path} not found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
